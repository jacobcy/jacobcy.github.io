---
title: ç”¨Pythonåšæ•°æ®åˆ†æï¼šPandaså®æˆ˜æŒ‡å—
date: 2025-09-05 15:00:00
updated: 2025-09-05 15:00:00
description: 'Pandasæ•°æ®åˆ†æå®Œå…¨æŒ‡å—ï¼Œä»æ•°æ®æ¸…æ´—ã€å¤„ç†åˆ°å¯è§†åŒ–çš„å®Œæ•´å®æˆ˜ï¼ŒåŒ…å«é‡åŒ–æŠ•èµ„ã€å•†ä¸šåˆ†æç­‰çœŸå®æ¡ˆä¾‹'
keywords: 'Python,Pandas,æ•°æ®åˆ†æ,æ•°æ®æ¸…æ´—,é‡åŒ–åˆ†æ,æ•°æ®å¯è§†åŒ–,DataFrame,å®æˆ˜æ•™ç¨‹'
tags: 
  - Python
  - Pandas
  - æ•°æ®åˆ†æ
  - é‡åŒ–åˆ†æ
categories:
  - æŠ€æœ¯ç¬”è®°
author: Yi Chen
toc: true
comments: true
---

> ğŸ "Pandasæ˜¯Pythonæ•°æ®ç§‘å­¦çš„åŸºçŸ³ã€‚"

## ğŸ¯ ä¸ºä»€ä¹ˆå­¦Pandas

ä½œä¸ºä¸€ä¸ªç¨‹åºå‘˜+æŠ•èµ„è€…ï¼Œæˆ‘æ¯å¤©éƒ½åœ¨å’Œæ•°æ®æ‰“äº¤é“ï¼š
- è‚¡ç¥¨ä»·æ ¼æ•°æ®
- æŠ•èµ„ç»„åˆè¡¨ç°
- ç½‘ç«™è®¿é—®ç»Ÿè®¡
- å’–å•¡å†²æ³¡è®°å½•

**Pandasè®©è¿™ä¸€åˆ‡å˜å¾—ç®€å•**ã€‚

## ğŸš€ å¿«é€Ÿå…¥é—¨

### å®‰è£…

```bash
pip install pandas numpy matplotlib seaborn
```

### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
import pandas as pd
import numpy as np

# 1. Series - ä¸€ç»´æ•°æ®
s = pd.Series([1, 3, 5, np.nan, 6, 8])
print(s)

# 2. DataFrame - äºŒç»´æ•°æ®
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': ['a', 'b', 'c', 'd'],
    'C': [1.0, 2.0, 3.0, 4.0]
})
print(df)
```

## ğŸ’¡ å®æˆ˜æ¡ˆä¾‹1ï¼šè‚¡ç¥¨æ•°æ®åˆ†æ

### è·å–æ•°æ®

```python
import akshare as ak
import pandas as pd

# è·å–è´µå·èŒ…å°å†å²æ•°æ®
df = ak.stock_zh_a_hist(
    symbol="600519",
    period="daily",
    start_date="20230101",
    end_date="20241231"
)

# æŸ¥çœ‹æ•°æ®ç»“æ„
print(df.head())
print(df.info())
```

### æ•°æ®æ¸…æ´—

```python
# é‡å‘½ååˆ—ï¼ˆè‹±æ–‡ï¼Œæ–¹ä¾¿åç»­å¤„ç†ï¼‰
df = df.rename(columns={
    'æ—¥æœŸ': 'date',
    'å¼€ç›˜': 'open',
    'æ”¶ç›˜': 'close',
    'æœ€é«˜': 'high',
    'æœ€ä½': 'low',
    'æˆäº¤é‡': 'volume'
})

# è½¬æ¢æ—¥æœŸæ ¼å¼
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# æ£€æŸ¥ç¼ºå¤±å€¼
print(df.isnull().sum())

# åˆ é™¤ç¼ºå¤±å€¼
df = df.dropna()

# æ•°æ®ç±»å‹è½¬æ¢
df['volume'] = df['volume'].astype(int)
```

### è®¡ç®—æŠ€æœ¯æŒ‡æ ‡

```python
# ç§»åŠ¨å¹³å‡çº¿
df['MA5'] = df['close'].rolling(window=5).mean()
df['MA20'] = df['close'].rolling(window=20).mean()
df['MA60'] = df['close'].rolling(window=60).mean()

# æ¶¨è·Œå¹…
df['returns'] = df['close'].pct_change()

# æ³¢åŠ¨ç‡ï¼ˆ20æ—¥ï¼‰
df['volatility'] = df['returns'].rolling(window=20).std() * np.sqrt(252)

# RSIæŒ‡æ ‡
def calculate_rsi(prices, window=14):
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df['RSI'] = calculate_rsi(df['close'])

print(df.tail())
```

### æ•°æ®åˆ†æ

```python
# åŸºç¡€ç»Ÿè®¡
print("åŸºç¡€ç»Ÿè®¡ï¼š")
print(df['returns'].describe())

# å¹´åŒ–æ”¶ç›Š
total_return = (df['close'].iloc[-1] / df['close'].iloc[0]) - 1
years = (df.index[-1] - df.index[0]).days / 365.25
annual_return = (1 + total_return) ** (1/years) - 1
print(f"\nå¹´åŒ–æ”¶ç›Šç‡: {annual_return:.2%}")

# æœ€å¤§å›æ’¤
cumulative = (1 + df['returns']).cumprod()
rolling_max = cumulative.expanding().max()
drawdown = (cumulative - rolling_max) / rolling_max
max_drawdown = drawdown.min()
print(f"æœ€å¤§å›æ’¤: {max_drawdown:.2%}")

# å¤æ™®æ¯”ç‡ï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡2%ï¼‰
excess_return = df['returns'].mean() * 252 - 0.02
sharpe = excess_return / (df['returns'].std() * np.sqrt(252))
print(f"å¤æ™®æ¯”ç‡: {sharpe:.2f}")
```

### å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# åˆ›å»ºå­å›¾
fig, axes = plt.subplots(3, 1, figsize=(12, 10))

# å›¾1ï¼šä»·æ ¼ä¸å‡çº¿
axes[0].plot(df.index, df['close'], label='Close', linewidth=1)
axes[0].plot(df.index, df['MA5'], label='MA5', alpha=0.7)
axes[0].plot(df.index, df['MA20'], label='MA20', alpha=0.7)
axes[0].set_title('Stock Price with Moving Averages')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# å›¾2ï¼šæ”¶ç›Šç‡åˆ†å¸ƒ
axes[1].hist(df['returns'].dropna(), bins=50, alpha=0.7, edgecolor='black')
axes[1].set_title('Returns Distribution')
axes[1].set_xlabel('Daily Returns')
axes[1].axvline(df['returns'].mean(), color='red', linestyle='--', label='Mean')
axes[1].legend()

# å›¾3ï¼šç´¯è®¡æ”¶ç›Š
cumulative_returns = (1 + df['returns']).cumprod()
axes[2].plot(df.index, cumulative_returns)
axes[2].set_title('Cumulative Returns')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## ğŸ’¡ å®æˆ˜æ¡ˆä¾‹2ï¼šæŠ•èµ„ç»„åˆåˆ†æ

### å¤šèµ„äº§æ•°æ®è·å–

```python
# è·å–å¤šåªè‚¡ç¥¨æ•°æ®
stocks = ['600519', '000858', '000333', '000002']  # èŒ…å°ã€äº”ç²®æ¶²ã€ç¾çš„ã€ä¸‡ç§‘
stock_names = ['èŒ…å°', 'äº”ç²®æ¶²', 'ç¾çš„', 'ä¸‡ç§‘']

data = {}
for symbol, name in zip(stocks, stock_names):
    df = ak.stock_zh_a_hist(symbol=symbol, period="daily", 
                           start_date="20230101", end_date="20241231")
    df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
    df.set_index('date', inplace=True)
    data[name] = df['æ”¶ç›˜']

# åˆå¹¶æˆDataFrame
prices = pd.DataFrame(data)
prices = prices.dropna()
print(prices.head())
```

### æ”¶ç›Šä¸é£é™©åˆ†æ

```python
# è®¡ç®—æ”¶ç›Šç‡
returns = prices.pct_change().dropna()

# å¹´åŒ–æ”¶ç›Š
annual_returns = returns.mean() * 252
print("å¹´åŒ–æ”¶ç›Šï¼š")
print(annual_returns)

# å¹´åŒ–æ³¢åŠ¨ç‡
annual_volatility = returns.std() * np.sqrt(252)
print("\nå¹´åŒ–æ³¢åŠ¨ç‡ï¼š")
print(annual_volatility)

# ç›¸å…³æ€§çŸ©é˜µ
correlation = returns.corr()
print("\nç›¸å…³æ€§çŸ©é˜µï¼š")
print(correlation)

# å¯è§†åŒ–ç›¸å…³æ€§
plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0,
            square=True, fmt='.2f')
plt.title('Stock Returns Correlation')
plt.show()
```

### æŠ•èµ„ç»„åˆä¼˜åŒ–

```python
# è®¡ç®—åæ–¹å·®çŸ©é˜µ
cov_matrix = returns.cov() * 252

# ç­‰æƒé‡ç»„åˆ
weights = np.array([0.25, 0.25, 0.25, 0.25])
portfolio_return = np.sum(annual_returns * weights)
portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
sharpe_ratio = (portfolio_return - 0.02) / portfolio_volatility

print(f"ç­‰æƒé‡ç»„åˆï¼š")
print(f"é¢„æœŸå¹´åŒ–æ”¶ç›Š: {portfolio_return:.2%}")
print(f"å¹´åŒ–æ³¢åŠ¨ç‡: {portfolio_volatility:.2%}")
print(f"å¤æ™®æ¯”ç‡: {sharpe_ratio:.2f}")

# ç»˜åˆ¶æœ‰æ•ˆå‰æ²¿
from scipy.optimize import minimize

def portfolio_performance(weights, returns, cov_matrix):
    port_return = np.sum(returns * weights)
    port_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    return port_return, port_volatility

def minimize_volatility(weights, returns, cov_matrix):
    return portfolio_performance(weights, returns, cov_matrix)[1]

# ç”Ÿæˆéšæœºç»„åˆ
num_portfolios = 1000
results = np.zeros((3, num_portfolios))
weight_array = []

for i in range(num_portfolios):
    weights = np.random.random(4)
    weights /= np.sum(weights)
    weight_array.append(weights)
    
    port_return, port_volatility = portfolio_performance(weights, annual_returns, cov_matrix)
    results[0, i] = port_volatility
    results[1, i] = port_return
    results[2, i] = (port_return - 0.02) / port_volatility

# ç»˜åˆ¶
plt.figure(figsize=(10, 6))
plt.scatter(results[0, :], results[1, :], c=results[2, :], 
           cmap='YlOrRd', marker='o', s=10, alpha=0.5)
plt.colorbar(label='Sharpe Ratio')
plt.xlabel('Volatility')
plt.ylabel('Expected Return')
plt.title('Efficient Frontier')
plt.grid(True, alpha=0.3)
plt.show()
```

## ğŸ’¡ å®æˆ˜æ¡ˆä¾‹3ï¼šæ•°æ®æ¸…æ´—æŠ€å·§

### å¤„ç†ç¼ºå¤±å€¼

```python
import pandas as pd
import numpy as np

# åˆ›å»ºç¤ºä¾‹æ•°æ®
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': ['a', 'b', 'c', np.nan, 'e'],
    'C': [1.0, 2.0, 3.0, 4.0, 5.0]
})

# æŸ¥çœ‹ç¼ºå¤±å€¼
print(df.isnull().sum())

# å¡«å……ç¼ºå¤±å€¼
df['A'].fillna(df['A'].mean(), inplace=True)  # ç”¨å‡å€¼å¡«å……
df['B'].fillna('unknown', inplace=True)  # ç”¨ç‰¹å®šå€¼å¡«å……

# åˆ é™¤ç¼ºå¤±å€¼
df_clean = df.dropna()  # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
df_clean = df.dropna(subset=['A', 'B'])  # åªåœ¨ç‰¹å®šåˆ—æ£€æŸ¥
```

### æ•°æ®ç±»å‹è½¬æ¢

```python
# å­—ç¬¦ä¸²è½¬æ—¥æœŸ
df['date'] = pd.to_datetime(df['date_string'])

# å­—ç¬¦ä¸²è½¬æ•°å€¼
df['numeric'] = pd.to_numeric(df['string_number'], errors='coerce')

# åˆ†ç±»å˜é‡
df['category'] = df['category'].astype('category')

# æå–æ—¥æœŸç‰¹å¾
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter
```

### æ•°æ®åˆå¹¶

```python
# åˆ›å»ºç¤ºä¾‹æ•°æ®
df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['B', 'C', 'D'], 'value2': [4, 5, 6]})

# ä¸åŒåˆå¹¶æ–¹å¼
inner = pd.merge(df1, df2, on='key', how='inner')  # äº¤é›†
outer = pd.merge(df1, df2, on='key', how='outer')  # å¹¶é›†
left = pd.merge(df1, df2, on='key', how='left')   # ä¿ç•™å·¦è¡¨
right = pd.merge(df1, df2, on='key', how='right')  # ä¿ç•™å³è¡¨

# æŒ‰ç´¢å¼•åˆå¹¶
df_merged = pd.concat([df1, df2], axis=1)  # æ¨ªå‘åˆå¹¶
df_stacked = pd.concat([df1, df2], axis=0)  # çºµå‘åˆå¹¶
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨å‘é‡åŒ–æ“ä½œ

```python
# æ…¢ï¼šå¾ªç¯
result = []
for i in range(len(df)):
    result.append(df.iloc[i]['A'] + df.iloc[i]['B'])

# å¿«ï¼šå‘é‡åŒ–
df['result'] = df['A'] + df['B']
```

### 2. ä½¿ç”¨Categoricalç±»å‹

```python
# å¦‚æœæœ‰å¤§é‡é‡å¤å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºcategory
df['category'] = df['category'].astype('category')

# å†…å­˜ä½¿ç”¨å‡å°‘80%+
```

### 3. åˆ†å—å¤„ç†å¤§æ–‡ä»¶

```python
# å¤„ç†å¤§CSVæ–‡ä»¶
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # å¤„ç†æ¯ä¸ªchunk
    processed = chunk.process()
    chunks.append(processed)

# åˆå¹¶ç»“æœ
df = pd.concat(chunks)
```

### 4. ä½¿ç”¨queryå’Œeval

```python
# æ¯”æ ‡å‡†ç´¢å¼•å¿«
df.query('A > 0 and B < 100')

# å¤æ‚è®¡ç®—
df.eval('C = A + B')
```

## ğŸ“Š å¸¸ç”¨ä»£ç æ¨¡æ¿

### æ¨¡æ¿1ï¼šæ•°æ®æ¢ç´¢

```python
def explore_data(df):
    """å¿«é€Ÿæ¢ç´¢æ•°æ®é›†"""
    print("æ•°æ®é›†å½¢çŠ¶:", df.shape)
    print("\næ•°æ®ç±»å‹:")
    print(df.dtypes)
    print("\nåŸºç¡€ç»Ÿè®¡:")
    print(df.describe())
    print("\nç¼ºå¤±å€¼:")
    print(df.isnull().sum())
    print("\né‡å¤è¡Œæ•°:", df.duplicated().sum())
```

### æ¨¡æ¿2ï¼šæ—¶é—´åºåˆ—å¤„ç†

```python
def resample_data(df, freq='M'):
    """é‡é‡‡æ ·æ—¶é—´åºåˆ—æ•°æ®"""
    return df.resample(freq).agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    })
```

### æ¨¡æ¿3ï¼šåˆ†ç»„ç»Ÿè®¡

```python
def group_analysis(df, group_col, value_col):
    """åˆ†ç»„ç»Ÿè®¡åˆ†æ"""
    return df.groupby(group_col)[value_col].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(2)
```

---

## ğŸ’­ å†™åœ¨æœ€å

Pandasæ˜¯æ•°æ®åˆ†æå¸ˆçš„"ç‘å£«å†›åˆ€"ã€‚

æŒæ¡å®ƒï¼Œä½ å¯ä»¥ï¼š
- å¤„ç†ä»»ä½•ç»“æ„åŒ–æ•°æ®
- å¿«é€Ÿè¿›è¡Œæ•°æ®æ¸…æ´—å’Œè½¬æ¢
- ç”Ÿæˆä¸“ä¸šçº§çš„åˆ†ææŠ¥å‘Š

**å­¦ä¹ å»ºè®®**ï¼š
1. å…ˆæŒæ¡åŸºç¡€ï¼ˆDataFrameã€Seriesï¼‰
2. é€šè¿‡å®æˆ˜é¡¹ç›®ç»ƒä¹ 
3. é‡åˆ°é—®é¢˜æŸ¥æ–‡æ¡£ï¼Œä¸è¦æ­»è®°API
4. å…³æ³¨æ€§èƒ½ä¼˜åŒ–ï¼ˆå¤§æ•°æ®åœºæ™¯ï¼‰

> "æ•°æ®æ˜¯æ–°æ—¶ä»£çš„çŸ³æ²¹ï¼ŒPandasæ˜¯ç‚¼æ²¹å‚ã€‚"

å¸Œæœ›è¿™ç¯‡æŒ‡å—å¯¹ä½ æœ‰å¸®åŠ©ï¼æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿äº¤æµã€‚ğŸ¼

---

*å†™äº 2025å¹´9æœˆ5æ—¥ | Pandaså®æˆ˜æ€»ç»“*
